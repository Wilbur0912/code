{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import shap\n",
    "from deeplift.visualization import viz_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "\n",
    "# CNN model\n",
    "class SpliceCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, kernel_size=5):\n",
    "        super(SpliceCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=kernel_size, padding=0)\n",
    "        self.fc1 = nn.Linear(8 * (input_size - kernel_size + 1), 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# nucleotide dataset class\n",
    "class SpliceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return seq, label\n",
    "\n",
    "\n",
    "# one-hot DNA encoder\n",
    "def encode_dna(seq):\n",
    "    encoding = {'A': [1,0,0,0], \n",
    "                'C': [0,1,0,0], \n",
    "                'G': [0,0,1,0], \n",
    "                'T': [0,0,0,1],\n",
    "                'D': [1/3,0,1/3,1/3],\n",
    "                'N': [1/4,1/4,1/4,1/4],\n",
    "                'S': [0,1/2,1/2,0],\n",
    "                'R': [1/2,0,1/2,0]}\n",
    "    \n",
    "    return np.array([encoding.get(base, [0,0,0,0]) for base in seq])\n",
    "\n",
    "\n",
    "# load and encode sequence data\n",
    "def load_data():\n",
    "    # fetch dataset\n",
    "    raw_sequences = fetch_ucirepo(id=69)\n",
    "\n",
    "    # extract DNA sequences and labels\n",
    "    sequences = raw_sequences.data.features.values\n",
    "    labels = raw_sequences.data.targets.values.reshape(-1,)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "\n",
    "    # to torch tensor\n",
    "    encoded_sequences = np.array([encode_dna(seq) for seq in sequences])\n",
    "    encoded_sequences = torch.FloatTensor(encoded_sequences).permute(0, 2, 1)\n",
    "    labels = torch.LongTensor(y)\n",
    "\n",
    "    return encoded_sequences, labels\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            # Get predicted class from outputs\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    classification_accuracy = 100 * correct / total\n",
    "    return classification_accuracy\n",
    "\n",
    "\n",
    "def plot_accuracies(train_accuracies, test_accuracies):\n",
    "    epochs = range(1, len(train_accuracies) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shuffle_several_times(s):\n",
    "    s = np.squeeze(s)\n",
    "    return dinuc_shuffle(s, num_shufs=100)\n",
    "\n",
    "\n",
    "def deeplift_analysis(model, encoded_sequences, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to CPU for SHAP compatibility (SHAP may not work properly with CUDA)\n",
    "    model = model.to('cpu')\n",
    "    \n",
    "    # Use DeepExplainer to compute SHAP values\n",
    "    explainer = shap.DeepExplainer(model, encoded_sequences[:100])  # Use a subset of sequences as background\n",
    "    shap_values = explainer.shap_values(encoded_sequences[:10])  # Explaining the first 10 test sequences\n",
    "\n",
    "    # Visualize the explanations for some test sequences\n",
    "    for i in range(len(shap_values)):\n",
    "        print(f\"Visualizing explanation for test sequence {i+1}\")\n",
    "        \n",
    "        # Project the SHAP values onto the base that is actually present in each position\n",
    "        shap_seq = shap_values[i].sum(axis=-1) * encoded_sequences[i].detach().numpy()\n",
    "        \n",
    "        # Convert the explanation to a form that viz_sequence can use\n",
    "        viz_sequence.plot_weights(shap_seq.squeeze(), subticks_frequency=20)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"DeepLIFT analysis and visualizations complete.\")\n",
    "\n",
    "\n",
    "def find_confident_predictions(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_sequences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_sequences.append(sequences.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_sequences = np.concatenate(all_sequences)\n",
    "\n",
    "    # Find the most confident correct and incorrect predictions for each class\n",
    "    confident_correct = {}\n",
    "    confident_incorrect = {}\n",
    "\n",
    "    for cls in range(3):  # 3 classes: Donor, Acceptor, Negative\n",
    "        # Find correct predictions\n",
    "        correct_mask = (all_preds == cls) & (all_labels == cls)\n",
    "        incorrect_mask = (all_preds == cls) & (all_labels != cls)\n",
    "\n",
    "        if correct_mask.sum() > 0:\n",
    "            # Most confident correct prediction\n",
    "            correct_probs = all_probs[correct_mask, cls]\n",
    "            confident_correct[cls] = np.argmax(correct_probs)\n",
    "\n",
    "        if incorrect_mask.sum() > 0:\n",
    "            # Most confident incorrect prediction\n",
    "            incorrect_probs = all_probs[incorrect_mask, cls]\n",
    "            confident_incorrect[cls] = np.argmax(incorrect_probs)\n",
    "\n",
    "    return confident_correct, confident_incorrect, all_sequences\n",
    "\n",
    "\n",
    "def visualize_confident_predictions(confident_correct, confident_incorrect, all_sequences, shap_values, encoded_sequences):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    classes = ['Donor', 'Acceptor', 'Negative']\n",
    "    \n",
    "    for i, cls in enumerate(classes):\n",
    "        # Confident correct prediction\n",
    "        if cls in confident_correct:\n",
    "            correct_idx = confident_correct[cls]\n",
    "            shap_seq = shap_values[correct_idx].sum(axis=-1) * encoded_sequences[correct_idx].detach().numpy()\n",
    "            viz_sequence.plot_weights(shap_seq.squeeze(), subticks_frequency=20, ax=axs[i, 0])\n",
    "            axs[i, 0].set_title(f'{cls}: Correct Confident Prediction')\n",
    "        \n",
    "        # Confident incorrect prediction\n",
    "        if cls in confident_incorrect:\n",
    "            incorrect_idx = confident_incorrect[cls]\n",
    "            shap_seq = shap_values[incorrect_idx].sum(axis=-1) * encoded_sequences[incorrect_idx].detach().numpy()\n",
    "            viz_sequence.plot_weights(shap_seq.squeeze(), subticks_frequency=20, ax=axs[i, 1])\n",
    "            axs[i, 1].set_title(f'{cls}: Incorrect Confident Prediction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load and preprocess data\n",
    "    encoded_sequences, labels = load_data()\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_sequences, labels, \n",
    "                                                        test_size=0.1, random_state=11)\n",
    "\n",
    "    # dataloaders\n",
    "    train_dataset = SpliceDataset(X_train, y_train)\n",
    "    test_dataset = SpliceDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # init model\n",
    "    model = SpliceCNN(input_size=60, num_classes=3, kernel_size=4).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # train model\n",
    "    num_epochs = 50\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_model(model, train_loader, criterion, optimizer, device)\n",
    "        # Calculate train and test accuracy\n",
    "        train_accuracy = evaluate_model(model, train_loader, device)\n",
    "        test_accuracy = evaluate_model(model, test_loader, device)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # plot accuracy over epochs\n",
    "    plot_accuracies(train_accuracies, test_accuracies)\n",
    "\n",
    "    # perform DeepLIFT analysis\n",
    "    deeplift_analysis(model, encoded_sequences, device)\n",
    "\n",
    "    # Find confident predictions and visualize them\n",
    "    confident_correct, confident_incorrect, all_sequences = find_confident_predictions(model, test_loader, device)\n",
    "    visualize_confident_predictions(confident_correct, confident_incorrect, all_sequences, encoded_sequences, encoded_sequences)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
